# nuplan/planning/simulation/main_callback/

**Tier 2 Documentation - Process-Level Simulation Orchestration Callbacks**

## Purpose & Scope

This module provides **process-level callbacks for multi-scenario simulation orchestration**, executing at simulation run boundaries (start/end of entire process) rather than individual scenario steps. Main callbacks handle **cross-scenario aggregation, metrics consolidation, validation, publishing, and competition tracking** - operations that require a global view of all simulation results.

**Key Distinction from `callback/` module:**

```
AbstractMainCallback (THIS MODULE)     vs.     AbstractCallback (callback/)
â”œâ”€ 2 hooks: start/end                         â”œâ”€ 8 hooks: per-scenario lifecycle
â”œâ”€ Runs ONCE per simulation job               â”œâ”€ Runs MANY times (per scenario/step)
â”œâ”€ Operates on aggregated results             â”œâ”€ Operates on live scenario state
â””â”€ Post-processing & publishing               â””â”€ Real-time metrics & serialization

Example timing:
  on_run_simulation_start()              on_initialization_start()
        â”‚                                      â”‚
        â”œâ”€ [Scenario 1]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  100 on_step_start/end calls
        â”œâ”€ [Scenario 2]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  100 on_step_start/end calls
        â””â”€ [Scenario N]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  100 on_step_start/end calls
        â”‚                                      â”‚
  on_run_simulation_end()                on_initialization_end()
    (aggregates ALL scenarios)            (finalizes single scenario)
```

**Typical use cases:**
- **Metric aggregation**: Consolidate per-scenario metric files into global parquet datasets
- **PDF reports**: Generate histogram summaries across all scenarios
- **Validation**: Create pass/fail markers for competition submissions
- **S3 publishing**: Upload results to cloud storage
- **Competition tracking**: Mark jobs as complete for scoring systems

**AIDEV-NOTE**: Main callbacks run AFTER all scenarios complete, making them ideal for operations requiring complete result sets (e.g., statistical aggregation, leaderboard updates).

---

## Key Abstractions

### AbstractMainCallback (14 lines)
**File**: `abstract_main_callback.py`

The minimal interface for process-level hooks:

```python
class AbstractMainCallback(abc.ABC):
    """Abstract class for main function callbacks."""

    def on_run_simulation_start(self) -> None:
        """Callback after the simulation function starts."""
        pass

    def on_run_simulation_end(self) -> None:
        """Callback before the simulation function ends."""
        pass
```

**Design rationale**: Only 2 hooks (vs. AbstractCallback's 8) because main callbacks operate on **completed results**, not live simulation state.

**Hook semantics**:
- `on_run_simulation_start()`: Called BEFORE any scenarios run (rare usage)
- `on_run_simulation_end()`: Called AFTER all scenarios complete (primary hook)

**When to use each hook:**
- **Start hook**: Time measurement (TimeCallback), resource initialization
- **End hook**: Aggregation, reporting, publishing (90% of use cases)

---

### MultiMainCallback (34 lines)
**File**: `multi_main_callback.py`

Composite pattern for orchestrating multiple callbacks in sequence:

```python
class MultiMainCallback(AbstractMainCallback):
    """Combines a set of of AbstractMainCallbacks."""

    def __init__(self, main_callbacks: List[AbstractMainCallback]):
        self._main_callbacks = main_callbacks

    def on_run_simulation_start(self) -> None:
        for main_callback in self._main_callbacks:
            main_callback.on_run_simulation_start()

    def on_run_simulation_end(self) -> None:
        for main_callback in self._main_callbacks:
            main_callback.on_run_simulation_end()
```

**Execution semantics:**
- Callbacks execute **sequentially** in list order (CRITICAL for correctness!)
- **No exception isolation** - if callback N crashes, callbacks N+1...M don't run
- **No parallel execution** - callbacks run one after another

**Typical callback ordering:**
```python
MultiMainCallback([
    MetricFileCallback(),         # 1. Aggregate scenario metrics â†’ parquet
    MetricAggregatorCallback(),   # 2. Run aggregators on parquet files
    MetricSummaryCallback(),      # 3. Render PDF summaries
    ValidationCallback(),         # 4. Create pass/fail markers
    PublisherCallback(),          # 5. Upload to S3
    CompletionCallback(),         # 6. Mark job complete
    TimeCallback()                # 7. Log total duration
])
```

**Why order matters:**
1. `MetricFileCallback` must run before `MetricAggregatorCallback` (aggregator needs parquet files)
2. `MetricAggregatorCallback` should run before `MetricSummaryCallback` (PDF includes aggregator results)
3. `ValidationCallback` should run before `PublisherCallback` (upload validation status)
4. `PublisherCallback` should run before `CompletionCallback` (ensure results uploaded before marking done)

**AIDEV-NOTE**: No runtime validation of dependencies - order bugs only discovered when callbacks execute!

---

### TimeCallback (28 lines)
**File**: `time_callback.py`

Measures and logs total simulation runtime:

```python
class TimeCallback(AbstractMainCallback):
    """Callback for tracking how long a simulation took to run."""

    def on_run_simulation_start(self) -> None:
        self._start_time = time.perf_counter()

    def on_run_simulation_end(self) -> None:
        elapsed_time_s = time.perf_counter() - self._start_time
        time_str = time.strftime("%H:%M:%SS", time.gmtime(elapsed_time_s))
        logger.info(f"Simulation duration: {time_str} [HH:MM:SS]")
```

**Usage**: Typically included in all simulation runs for performance tracking.

**Output format**: `HH:MM:SS` using GMT time conversion (e.g., `02:15:37` for 2h 15m 37s)

**AIDEV-NOTE**: One of the few callbacks that uses `on_run_simulation_start()` - most only implement end hook.

---

### MetricFileCallback (80 lines)
**File**: `metric_file_callback.py`

Aggregates per-scenario metric files (msgpack) into consolidated parquet files:

```python
class MetricFileCallback(AbstractMainCallback):
    """Callback to handle metric files at the end of process."""

    def __init__(
        self,
        metric_file_output_path: str,
        scenario_metric_paths: List[str],
        delete_scenario_metric_files: bool = False
    ):
        self._metric_file_output_path = Path(metric_file_output_path)
        self._scenario_metric_paths = [Path(p) for p in scenario_metric_paths]
        self._delete_scenario_metric_files = delete_scenario_metric_files
```

**Data flow:**
```
Input (per-scenario):
  scenario_metrics/
  â”œâ”€ scenario_001/metrics.msgpack.xz  (JSON pickle, ~100KB)
  â”œâ”€ scenario_002/metrics.msgpack.xz
  â””â”€ scenario_N/metrics.msgpack.xz

Output (consolidated):
  metrics/
  â”œâ”€ ego_acceleration.parquet         (All scenarios, ~5MB)
  â”œâ”€ collision_free.parquet
  â””â”€ drivable_area_compliance.parquet
```

**Algorithm:**
1. Recursively search `scenario_metric_paths` for `*.msgpack.xz` files
2. Load each file as `MetricStatisticsDataFrame`
3. Group by `metric_statistics_name` (e.g., "ego_acceleration")
4. Concatenate all dataframes for each metric type
5. Save to `{metric_file_output_path}/{metric_name}.parquet`
6. Optionally delete source files (`delete_scenario_metric_files=True`)

**Performance**: ~30-60 seconds for 100 scenarios Ã— 50 metrics

**AIDEV-NOTE**: Must run BEFORE `MetricAggregatorCallback` and `MetricSummaryCallback` - they depend on consolidated parquet files!

---

### MetricAggregatorCallback (70 lines)
**File**: `metric_aggregator_callback.py`

Runs metric aggregators on consolidated parquet files:

```python
class MetricAggregatorCallback(AbstractMainCallback):
    """Callback to aggregate metrics after the simulation ends."""

    def __init__(
        self,
        metric_save_path: str,
        metric_aggregators: List[AbstractMetricAggregator]
    ):
        self._metric_save_path = Path(metric_save_path)
        self._metric_aggregators = metric_aggregators
```

**Data flow:**
```
Input:
  metrics/
  â”œâ”€ ego_acceleration.parquet
  â””â”€ collision_free.parquet

Processing:
  metric_aggregators = [
    WeightedAverageMetricAggregator(),
    ScenarioTypeMetricAggregator()
  ]

Output:
  aggregator/
  â”œâ”€ weighted_average_ego_acceleration.parquet
  â””â”€ scenario_type_collision_free.parquet
```

**Algorithm:**
1. Find all `*.parquet` files in `metric_save_path`
2. Load each as `MetricStatisticsDataFrame`
3. Filter by `challenge` name (if aggregator specifies one)
4. Call `aggregator(metric_dataframes={...})` for each aggregator
5. Aggregator saves computed results to disk

**Common aggregators:**
- `WeightedAverageMetricAggregator`: Scenario-weighted metric averages
- `ScenarioTypeMetricAggregator`: Per-scenario-type breakdowns

**Error handling:**
- Logs warning if parquet file can't be loaded (continues execution)
- Logs warning if no metric files found
- Continues to next aggregator even if one fails

**Performance**: ~60-120 seconds for complex aggregators on 1000 scenarios

**AIDEV-NOTE**: Aggregators are plugins - easy to add custom logic by implementing `AbstractMetricAggregator`.

---

### MetricSummaryCallback (345 lines)
**File**: `metric_summary_callback.py`

Renders aggregated metrics into multi-page PDF reports with histograms:

```python
class MetricSummaryCallback(AbstractMainCallback):
    """Callback to render histograms for metrics and metric aggregator."""

    def __init__(
        self,
        metric_save_path: str,
        metric_aggregator_save_path: str,
        summary_output_path: str,
        pdf_file_name: str,
        num_bins: int = 20,
    ):
        self._metric_save_path = Path(metric_save_path)
        self._metric_aggregator_save_path = Path(metric_aggregator_save_path)
        self._summary_output_path = Path(summary_output_path)
        self._pdf_file_name = pdf_file_name
        self._num_bins = num_bins
```

**Rendering pipeline:**
```
1. Load metrics/*.parquet + aggregator/*.parquet
2. Aggregate histogram data (bins, frequencies)
3. Compute histogram edges (num_bins)
4. Render matplotlib histograms (2 columns per page)
5. Save all plots to multi-page PDF
```

**Histogram types:**
- **Continuous metrics** (e.g., acceleration): Standard histogram with bins
- **Count metrics** (e.g., num_agents): Bar chart with discrete values
- **Boolean metrics** (e.g., is_comfortable): Bar chart (True/False bars)

**Color management:**
```python
# 3 color palettes (Set1, Set2, Set3) â†’ 36 unique colors
color_palette = cmap.get_cmap('Set1').colors + \
                cmap.get_cmap('Set2').colors + \
                cmap.get_cmap('Set3').colors

# Planner colors persist across metrics for consistency
planner_color_maps[planner_name] = color_choices[index % len(color_choices)]
```

**PDF layout:**
- **Columns**: 2 per page
- **Rows**: Auto-calculated based on number of statistics
- **Figure size**: 6-24 inches (scales with metric count)
- **Y-axis**: Percentage (0-100%)

**Performance**: **5-10 minutes** for 50 metrics (slowest callback!)

**Bottleneck**: Matplotlib rendering (`fig.savefig()`)

**AIDEV-NOTE**: Most complex callback (345 lines) - consider refactoring histogram rendering into separate module.

---

### ValidationCallback (54 lines)
**File**: `validation_callback.py`

Creates pass/fail marker files based on simulation success:

```python
class ValidationCallback(AbstractMainCallback):
    """Callback checking if a validation simulation was successful or not."""

    def __init__(self, output_dir: str, validation_dir_name: str):
        self.output_dir = Path(output_dir)
        self._validation_dir_name = validation_dir_name

    def on_run_simulation_end(self) -> None:
        if _validation_succeeded(self.output_dir):
            filename = 'passed.txt'
        else:
            filename = 'failed.txt'
        # Create empty file
        with (self.output_dir / self._validation_dir_name / filename).open('w'):
            pass
```

**Validation logic:**
```python
def _validation_succeeded(source_folder_path: Path) -> bool:
    df = pd.read_parquet(f'{source_folder_path}/runner_report.parquet')
    return bool(np.all(df['succeeded'].values))
```

**Semantics:**
- **Passed**: All scenarios succeeded (no crashes, no metric violations)
- **Failed**: At least one scenario failed
- **Failed**: `runner_report.parquet` missing (treated as failure)

**Output structure:**
```
output_dir/
â”œâ”€ validation/
â”‚  â”œâ”€ passed.txt   # Empty file if all succeeded
â”‚  â””â”€ failed.txt   # Empty file if any failed
â””â”€ runner_report.parquet
```

**Use case**: Competition validation runs check for `passed.txt` to confirm planner correctness.

**AIDEV-NOTE**: Empty file semantics - only filename matters (passed vs failed), not contents.

---

### PublisherCallback (106 lines)
**File**: `publisher_callback.py`

Uploads simulation artifacts to S3:

```python
@dataclass
class UploadConfig:
    """Config specifying files to be uploaded and their target paths."""
    name: str
    local_path: Path
    remote_path: Path

class PublisherCallback(AbstractMainCallback):
    """Callback publishing data to S3"""

    def __init__(
        self,
        uploads: Dict[str, Any],
        s3_client: Optional[boto3.client],
        s3_bucket: str,
        remote_prefix: Optional[List[str]],
    ):
        self._s3_client = s3_client or get_s3_client()
        self._s3_bucket = s3_bucket.strip('s3://') if s3_bucket.startswith('s3://') else s3_bucket
        self._upload_targets: List[UploadConfig] = []

        for name, upload_data in uploads.items():
            if upload_data["upload"]:
                self._upload_targets.append(UploadConfig(
                    name=name,
                    local_path=Path(upload_data["save_path"]),
                    remote_path=Path(upload_data.get("remote_path") or "")
                ))
```

**Upload config format:**
```python
uploads = {
    "metrics": {
        "upload": True,
        "save_path": "/tmp/exp/metrics",
        "remote_path": "results/metrics"
    },
    "nuboard": {
        "upload": False,  # Skip this upload
        "save_path": "/tmp/exp/nuboard"
    }
}
```

**Upload algorithm:**
1. For each upload target where `upload=True`:
2. Recursively list all files in `local_path` (via `list_files()`)
3. For each file, compute S3 key: `remote_prefix / remote_path / file`
4. Upload: `s3_client.upload_file(local_file, bucket, key)`

**S3 key construction:**
```python
# Example:
local_path = "/tmp/exp/metrics/ego_acceleration.parquet"
remote_prefix = ["submissions", "user123"]
remote_path = "results/metrics"

# S3 key: submissions/user123/results/metrics/ego_acceleration.parquet
```

**Performance**: 2-10 minutes depending on file sizes and network bandwidth

**AIDEV-NOTE**: Only uploads files, not directories - S3 doesn't have directory concept!

---

### CompletionCallback (46 lines)
**File**: `completion_callback.py`

Creates completion markers for competition tracking:

```python
class CompletionCallback(AbstractMainCallback):
    """Callback that creates a token file to mark that the simulation instance finished the job."""

    def __init__(self, output_dir: str, challenge_name: str):
        self._bucket = os.getenv("NUPLAN_SERVER_S3_ROOT_URL")
        assert self._bucket, "Target bucket must be specified!"

        instance_id = os.getenv("SCENARIO_FILTER_ID", "0")
        task_id = '_'.join([challenge_name, instance_id])
        self._completion_dir = Path(output_dir, 'simulation-results', task_id)

    def on_run_simulation_end(self) -> None:
        self._write_empty_file(self._completion_dir, 'completed.txt')
```

**Environment variables:**
- `NUPLAN_SERVER_S3_ROOT_URL`: S3 bucket for competition submissions (**REQUIRED!**)
- `SCENARIO_FILTER_ID`: Instance ID for parallel simulation splits (default: "0")

**Output structure:**
```
output_dir/
â””â”€ simulation-results/
   â””â”€ {challenge_name}_{instance_id}/
      â””â”€ completed.txt   # Empty marker file
```

**Use case**: Competition server polls S3 for `completed.txt` to know when jobs finish.

**AIDEV-NOTE**: Assertion on missing `NUPLAN_SERVER_S3_ROOT_URL` crashes during `__init__` - only use in competition mode!

---

## Architecture & Data Flow

### Complete Callback Execution Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  run_simulation.py                          â”‚
â”‚                                                              â”‚
â”‚  1. Build simulation runners (per scenario)                 â”‚
â”‚  2. Build main callbacks (MultiMainCallback)                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ on_run_simulation_start()                              â”‚ â”‚
â”‚  â”‚   â””â”€ TimeCallback: Record start time                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Run N scenarios (with AbstractCallback per-step hooks) â”‚ â”‚
â”‚  â”‚   â”œâ”€ Scenario 1: collect metrics â†’ scenario_metrics/1/â”‚ â”‚
â”‚  â”‚   â”œâ”€ Scenario 2: collect metrics â†’ scenario_metrics/2/â”‚ â”‚
â”‚  â”‚   â””â”€ Scenario N: collect metrics â†’ scenario_metrics/N/â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ on_run_simulation_end()                                â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 1. MetricFileCallback (30-60s)                    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Load scenario_metrics/*/metrics.msgpack.xz  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Group by metric type                        â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Save to metrics/{metric_name}.parquet       â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                          â”‚                              â”‚ â”‚
â”‚  â”‚                          â–¼                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 2. MetricAggregatorCallback (60-120s)             â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Load metrics/{metric_name}.parquet          â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Run WeightedAverageMetricAggregator         â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Save to aggregator/{aggregator_name}.parquetâ”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                          â”‚                              â”‚ â”‚
â”‚  â”‚                          â–¼                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 3. MetricSummaryCallback (5-10min) âš ï¸ SLOWEST     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Load metrics/*.parquet                      â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Load aggregator/*.parquet                   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Render matplotlib histograms                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Save to summary/report.pdf                  â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                          â”‚                              â”‚ â”‚
â”‚  â”‚                          â–¼                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 4. ValidationCallback (1-5s)                      â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Read runner_report.parquet                  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ Check "succeeded" column                    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Create validation/{passed|failed}.txt       â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                          â”‚                              â”‚ â”‚
â”‚  â”‚                          â–¼                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 5. PublisherCallback (2-10min)                    â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â”œâ”€ List files in metrics/, summary/, etc.      â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Upload to S3: s3://bucket/prefix/...        â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                          â”‚                              â”‚ â”‚
â”‚  â”‚                          â–¼                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 6. CompletionCallback (<1s)                       â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Create simulation-results/{task}/completed  â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                          â”‚                              â”‚ â”‚
â”‚  â”‚                          â–¼                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ 7. TimeCallback (<1ms)                            â”‚ â”‚ â”‚
â”‚  â”‚  â”‚    â””â”€ Log elapsed time (HH:MM:SS)                 â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total pipeline duration**: 10-20 minutes for 100 scenarios with standard callbacks

**Critical ordering constraints:**
1. `MetricFileCallback` **MUST** run before `MetricAggregatorCallback`
2. `MetricAggregatorCallback` **SHOULD** run before `MetricSummaryCallback`
3. `ValidationCallback` **SHOULD** run before `PublisherCallback`
4. `PublisherCallback` **SHOULD** run before `CompletionCallback`
5. `TimeCallback` typically runs **LAST** (log total duration including callbacks)

**AIDEV-NOTE**: MultiMainCallback has NO exception handling - if callback N crashes, callbacks N+1...M don't run!

---

## Dependencies

### Internal nuPlan Modules
- `nuplan.planning.simulation.callback.abstract_callback` - Per-scenario callbacks (different abstraction!)
- `nuplan.planning.metrics.aggregator.abstract_metric_aggregator` - Aggregator plugin interface
- `nuplan.planning.metrics.metric_dataframe` - MetricStatisticsDataFrame
- `nuplan.planning.metrics.metric_engine` - JSON_FILE_EXTENSION constant
- `nuplan.planning.nuboard.tabs.config.histogram_tab_config` - Histogram styling config
- `nuplan.planning.nuboard.utils.nuboard_histogram_utils` - Histogram data processing
- `nuplan.planning.nuboard.utils.utils` - Metric readers

### External Dependencies
- `nuplan.common.utils.s3_utils` - S3 path detection, client creation
- `nuplan.common.utils.io_utils` - File listing, path utilities

### Standard Library
- `abc` - Abstract base class
- `logging` - Logger instances
- `time` - Timing measurements
- `pathlib` - Path manipulation
- `os` - Environment variables
- `math` - Ceiling function for subplot layout
- `collections.defaultdict` - Grouping metrics
- `dataclasses` - UploadConfig

### Third-Party
- `pandas` - Parquet I/O, DataFrame operations
- `numpy` - Histogram bins, array operations
- `matplotlib` - PDF rendering, histogram plots
- `tqdm` - Progress bars
- `boto3` - S3 client

---

## Dependents

### Direct Consumers
- `nuplan.planning.script.run_simulation` - Main entry point
- `nuplan.planning.script.builders.main_callback_builder` - Hydra config â†’ callback objects
- `nuplan.planning.simulation.runner.simulations_runner` - Executes callback hooks

### Indirect Consumers
- Competition submission pipelines
- Batch metric aggregation jobs
- Automated validation systems
- Leaderboard scoring systems

---

## Common Usage Patterns

### Pattern 1: Basic Local Simulation with Metrics
```python
from nuplan.planning.simulation.main_callback import (
    TimeCallback,
    MetricFileCallback,
    MetricAggregatorCallback,
    MultiMainCallback
)

# Build callbacks
time_cb = TimeCallback()
metric_file_cb = MetricFileCallback(
    metric_file_output_path="/tmp/exp/metrics",
    scenario_metric_paths=["/tmp/exp/scenario_metrics"],
    delete_scenario_metric_files=True  # Save disk space
)
aggregator_cb = MetricAggregatorCallback(
    metric_save_path="/tmp/exp/metrics",
    metric_aggregators=[WeightedAverageMetricAggregator()]
)

# Compose callbacks (ORDER MATTERS!)
main_callback = MultiMainCallback([
    metric_file_cb,      # 1. Consolidate scenario metrics
    aggregator_cb,       # 2. Run aggregators
    time_cb              # 3. Log timing
])

# Execute simulation
main_callback.on_run_simulation_start()
# ... run scenarios ...
main_callback.on_run_simulation_end()
```

**When to use**: Local development, quick experiments

---

### Pattern 2: Full Pipeline with PDF Reports
```python
from nuplan.planning.simulation.main_callback import (
    MetricSummaryCallback,
)

# Add PDF generation to pipeline
summary_cb = MetricSummaryCallback(
    metric_save_path="/tmp/exp/metrics",
    metric_aggregator_save_path="/tmp/exp/aggregator",
    summary_output_path="/tmp/exp/summary",
    pdf_file_name="metrics_report.pdf",
    num_bins=20
)

main_callback = MultiMainCallback([
    metric_file_cb,
    aggregator_cb,
    summary_cb,          # Generate PDF (slow!)
    time_cb
])

# After simulation, open PDF
# /tmp/exp/summary/metrics_report.pdf
```

**When to use**: Final analysis, presentation materials

**AIDEV-NOTE**: PDF generation adds 5-10 minutes - skip for quick iterations!

---

### Pattern 3: Competition Submission
```python
import os

# Set required environment variables
os.environ["NUPLAN_SERVER_S3_ROOT_URL"] = "s3://nuplan-competition"
os.environ["SCENARIO_FILTER_ID"] = "42"

# Build competition callbacks
validation_cb = ValidationCallback(
    output_dir="/tmp/exp",
    validation_dir_name="validation"
)
publisher_cb = PublisherCallback(
    uploads={
        "metrics": {
            "upload": True,
            "save_path": "/tmp/exp/metrics",
            "remote_path": "results/metrics"
        },
        "validation": {
            "upload": True,
            "save_path": "/tmp/exp/validation",
            "remote_path": "validation"
        }
    },
    s3_client=None,  # Uses default S3 client from AWS credentials
    s3_bucket="nuplan-competition",
    remote_prefix=["submissions", "user123"]
)
completion_cb = CompletionCallback(
    output_dir="/tmp/exp",
    challenge_name="open_loop_boxes"
)

# Full competition pipeline
main_callback = MultiMainCallback([
    metric_file_cb,
    aggregator_cb,
    validation_cb,     # Create pass/fail marker
    publisher_cb,      # Upload to S3
    completion_cb,     # Mark job complete
    time_cb
])
```

**When to use**: Official competition submissions

**AIDEV-NOTE**: CompletionCallback crashes if `NUPLAN_SERVER_S3_ROOT_URL` not set!

---

### Pattern 4: Hydra Configuration (Production)
Most users don't instantiate callbacks manually - Hydra does it:

```yaml
# config/main_callback/default_main_callback.yaml
_target_: nuplan.planning.simulation.main_callback.multi_main_callback.MultiMainCallback
main_callbacks:
  - _target_: nuplan.planning.simulation.main_callback.metric_file_callback.MetricFileCallback
    metric_file_output_path: ${output_dir}/metrics
    scenario_metric_paths: ${scenario_metric_paths}
    delete_scenario_metric_files: true

  - _target_: nuplan.planning.simulation.main_callback.metric_aggregator_callback.MetricAggregatorCallback
    metric_save_path: ${output_dir}/metrics
    metric_aggregators: ${metric_aggregators}

  - _target_: nuplan.planning.simulation.main_callback.time_callback.TimeCallback
```

**AIDEV-NOTE**: Hydra instantiation is preferred - manual callback construction only needed for custom experiments.

---

### Pattern 5: Custom Callback Implementation
```python
from nuplan.planning.simulation.main_callback.abstract_main_callback import AbstractMainCallback
from pathlib import Path
import pandas as pd

class CustomMetricExporter(AbstractMainCallback):
    """Export metrics to custom format (e.g., TensorBoard, CSV)."""

    def __init__(self, metric_path: str, export_dir: str):
        self._metric_path = Path(metric_path)
        self._export_dir = Path(export_dir)

    def on_run_simulation_end(self) -> None:
        # Load consolidated metrics
        for metric_file in self._metric_path.glob("*.parquet"):
            df = pd.read_parquet(metric_file)

            # Export to custom format
            csv_path = self._export_dir / f"{metric_file.stem}.csv"
            df.to_csv(csv_path, index=False)

# Add to callback list
main_callback = MultiMainCallback([
    metric_file_cb,
    CustomMetricExporter(
        metric_path="/tmp/exp/metrics",
        export_dir="/tmp/exp/csv"
    ),
    time_cb
])
```

**When to use**: Custom export formats, integration with other tools

---

## Gotchas & Pitfalls

### Gotcha 1: Callback Order is CRITICAL âš ï¸
**Problem**: Callbacks depend on each other's outputs
**Impact**: FileNotFoundError or missing data

```python
# âŒ WRONG - aggregator runs before files exist
main_callback = MultiMainCallback([
    aggregator_cb,      # Tries to load parquet files...
    metric_file_cb      # ...that haven't been created yet!
])

# âœ… CORRECT
main_callback = MultiMainCallback([
    metric_file_cb,     # Creates parquet files
    aggregator_cb       # Then aggregates them
])
```

**AIDEV-NOTE**: No runtime validation of callback dependencies - order bugs only discovered when callbacks execute!

---

### Gotcha 2: MultiMainCallback Has No Exception Isolation
**Problem**: If one callback crashes, subsequent callbacks don't run
**Impact**: Partial results uploaded, completion marker not created

```python
main_callback = MultiMainCallback([
    metric_file_cb,     # âœ… Runs successfully
    aggregator_cb,      # ğŸ’¥ Crashes (missing file)
    publisher_cb,       # âŒ Never runs!
    completion_cb       # âŒ Never runs!
])
```

**Solution**: Wrap callbacks with exception handling:
```python
class ResilientCallback(AbstractMainCallback):
    def __init__(self, callback: AbstractMainCallback):
        self._callback = callback

    def on_run_simulation_end(self) -> None:
        try:
            self._callback.on_run_simulation_end()
        except Exception as e:
            logger.error(f"Callback {self._callback.__class__.__name__} failed: {e}")
            # Continue to next callback
```

---

### Gotcha 3: CompletionCallback Requires Environment Variables
**Problem**: Missing `NUPLAN_SERVER_S3_ROOT_URL` â†’ AssertionError during `__init__`
**Impact**: Simulation crashes before running any scenarios

```python
# âŒ Crashes if env var not set
completion_cb = CompletionCallback(output_dir="/tmp", challenge_name="test")

# âœ… Conditional inclusion
callbacks = [metric_file_cb, aggregator_cb, time_cb]
if os.getenv("NUPLAN_SERVER_S3_ROOT_URL"):
    callbacks.append(CompletionCallback(output_dir="/tmp", challenge_name="test"))
main_callback = MultiMainCallback(callbacks)
```

**AIDEV-NOTE**: Constructor assertions are bad practice - should raise ValueError with helpful message instead.

---

### Gotcha 4: S3 Paths Not Universally Supported
**Problem**: Some callbacks support S3 paths, others don't
**Impact**: Different behavior for S3 vs local paths

**S3 support status:**
- âœ… `MetricFileCallback`: Checks `is_s3_path()` before mkdir
- âœ… `ValidationCallback`: Checks `is_s3_path()` before mkdir
- âœ… `CompletionCallback`: Checks `is_s3_path()` before mkdir
- âŒ `MetricSummaryCallback`: No S3 support (crashes on S3 paths!)
- N/A `TimeCallback`: No file I/O

**AIDEV-NOTE**: MetricSummaryCallback doesn't support S3 paths - will crash if output_path is S3!

---

### Gotcha 5: MetricFileCallback Silently Skips Bad Files
**Problem**: Parquet load failures caught but only logged (not raised)
**Impact**: Missing metrics in aggregation, no error raised

```python
# In metric_file_callback.py:54
try:
    metric_statistic_dataframe = MetricStatisticsDataFrame.load_parquet(file)
except (FileNotFoundError, Exception) as e:
    logger.info(f"Cannot load the file: {file}, error: {e}")  # Just log!
    # File skipped, aggregation continues
```

**When this happens:**
- Corrupted parquet file
- Wrong file format
- Permissions error

**AIDEV-NOTE**: Catch-all `Exception` is dangerous - should catch specific exceptions (OSError, ArrowError).

---

### Gotcha 6: delete_scenario_metric_files is Irreversible
**Problem**: MetricFileCallback can delete source files after aggregation
**Impact**: Cannot re-run aggregation without re-running simulation

```python
metric_file_cb = MetricFileCallback(
    ...,
    delete_scenario_metric_files=True  # âš ï¸ Permanent deletion!
)
```

**When to use:**
- âœ… Production runs (save disk space)
- âŒ Development (may need to re-aggregate with different settings)

**AIDEV-NOTE**: Deletion happens during `on_run_simulation_end()` - if callback crashes mid-execution, some files deleted, some not!

---

### Gotcha 7: PublisherCallback Only Uploads Files, Not Directories
**Problem**: Empty directories not uploaded to S3
**Impact**: Directory structure not preserved

```python
# Local structure:
# /tmp/exp/metrics/
# â”œâ”€â”€ scenario_type_1/   # Empty directory
# â””â”€â”€ ego_acceleration.parquet

# After upload to S3:
# s3://bucket/metrics/ego_acceleration.parquet
# (scenario_type_1/ directory not created)
```

**AIDEV-NOTE**: `list_files()` explicitly skips directories (publisher_callback.py:27).

---

### Gotcha 8: Timing Logs Use GMT Time Format
**Problem**: `time.gmtime()` converts seconds to HH:MM:SS in GMT
**Impact**: Confusing for developers in other timezones

```python
elapsed_time_s = 7265  # 2 hours, 1 minute, 5 seconds
time_str = time.strftime("%H:%M:%S", time.gmtime(elapsed_time_s))
# Result: "02:01:05" (correct, but looks like GMT timestamp)
```

**AIDEV-NOTE**: This is fine for elapsed duration (no timezone needed) but misleading function name.

---

### Gotcha 9: MetricSummaryCallback Loads All Metrics into Memory
**Problem**: All parquet files loaded simultaneously (not streamed)
**Impact**: OOM for large experiments (1000+ scenarios, 50+ metrics)

```python
# In metric_summary_callback.py:318
self._metric_statistics_dataframes = self._read_metric_parquet_files(...)
# All metrics in memory at once!
```

**Memory usage**: 1000 scenarios Ã— 50 metrics Ã— 1 MB each = **50 GB RAM**

**AIDEV-NOTE**: Consider streaming parquet files or processing in batches for large-scale experiments.

---

### Gotcha 10: ValidationCallback Returns False on Missing Report
**Problem**: `_validation_succeeded()` returns False instead of raising exception
**Impact**: Validation failures indistinguishable from missing reports

```python
# In validation_callback.py:19-23
try:
    df = pd.read_parquet(f'{source_folder_path}/runner_report.parquet')
except FileNotFoundError:
    logger.warning("No runners report file found!")
    return False  # Same as validation failure!
```

**Better design**: Raise exception for missing file, return False only for actual failures.

---

### Gotcha 11: Publisher S3 Bucket Name Normalization
**Problem**: Bucket name silently stripped of `s3://` prefix
**Impact**: Unexpected behavior if bucket name actually starts with "s3://"

```python
# In publisher_callback.py:62
self._s3_bucket = s3_bucket.strip('s3://') if s3_bucket.startswith('s3://') else s3_bucket
```

**Edge case**: If someone names their bucket "s3://my-bucket" (unlikely), this breaks.

**AIDEV-NOTE**: Should use regex or urllib.parse for proper S3 URL parsing.

---

### Gotcha 12: MetricAggregatorCallback Challenge Name Filtering
**Problem**: Challenge name must be substring of file path (not just filename)
**Impact**: Aggregator misses files if challenge name placement is wrong

```python
# In metric_aggregator_callback.py:46
challenge_metrics = [path for path in metrics if metric_aggregator.challenge in str(path)]
```

**Example:**
```python
# âœ… Matches
metric_aggregator.challenge = "open_loop"
path = "/tmp/exp/metrics/open_loop_boxes/ego_acceleration.parquet"

# âŒ Doesn't match
metric_aggregator.challenge = "open_loop"
path = "/tmp/exp/metrics/ego_acceleration_open_loop.parquet"  # Challenge in filename, not directory
```

**AIDEV-NOTE**: Fragile filtering logic - better to use metadata in parquet file itself.

---

### Gotcha 13: Histogram Rendering Can Timeout
**Problem**: Matplotlib rendering is slow for many metrics (100+ subplots)
**Impact**: Callback appears to hang, no progress indication

```python
# In metric_summary_callback.py:243
for histogram_title, histogram_data_list in tqdm(histogram_data_dict.items(), desc='Rendering histograms'):
    # Can take 10+ minutes for large datasets
```

**AIDEV-NOTE**: tqdm helps but doesn't prevent timeout - consider parallel rendering or skip PDF for large experiments.

---

### Gotcha 14: Empty File Semantics in Validation/Completion
**Problem**: Callbacks create empty files (0 bytes) as markers
**Impact**: Cannot store metadata in marker files

```python
# In completion_callback.py:43
with (path / filename).open('w'):
    pass  # Empty file
```

**Better design**: Write JSON with metadata:
```python
with (path / filename).open('w') as f:
    json.dump({"timestamp": time.time(), "version": "1.0"}, f)
```

---

### Gotcha 15: Color Palette Cycling After 36 Planners
**Problem**: After 36 planners, colors repeat (3 palettes Ã— 12 colors each)
**Impact**: Color collisions in PDF summaries

```python
# In metric_summary_callback.py:63-64
color_palette = cmap.get_cmap('Set1').colors + \
                cmap.get_cmap('Set2').colors + \
                cmap.get_cmap('Set3').colors

# In metric_summary_callback.py:140
planner_color = color_choices[index % len(color_choices)]
```

**AIDEV-NOTE**: Consider generating distinct colors dynamically (e.g., HSV spacing) for >36 planners.

---

### Gotcha 16: No Callback Dependency Validation
**Problem**: MultiMainCallback doesn't validate callback dependencies
**Impact**: Order bugs only discovered at runtime

**AIDEV-TODO**: Implement dependency graph validation:
```python
class CallbackDependencyGraph:
    dependencies = {
        MetricAggregatorCallback: [MetricFileCallback],
        MetricSummaryCallback: [MetricFileCallback, MetricAggregatorCallback],
        PublisherCallback: [ValidationCallback],
    }

    @staticmethod
    def validate_order(callbacks: List[AbstractMainCallback]) -> None:
        seen = set()
        for cb in callbacks:
            for dep in CallbackDependencyGraph.dependencies.get(type(cb), []):
                if dep not in seen:
                    raise ValueError(f"{cb.__class__.__name__} requires {dep.__name__} to run first!")
            seen.add(type(cb))
```

---

## Performance Benchmarks

### Timing by Callback (100 scenarios, 50 metrics)

| Callback                     | Duration  | Bottleneck              | Optimization Priority |
|------------------------------|-----------|-------------------------|-----------------------|
| `MetricSummaryCallback`      | 5-10 min  | Matplotlib PDF rendering| **HIGH** âš ï¸           |
| `PublisherCallback`          | 2-10 min  | Network I/O (S3 upload) | Medium                |
| `MetricAggregatorCallback`   | 1-2 min   | Aggregator computation  | Low                   |
| `MetricFileCallback`         | 30-60 sec | Parquet I/O             | Low                   |
| `ValidationCallback`         | 1-5 sec   | Parquet read            | None                  |
| `CompletionCallback`         | <1 sec    | File write              | None                  |
| `TimeCallback`               | <1 ms     | Logging                 | None                  |

**Total pipeline**: 10-20 minutes for typical experiment

**Optimization strategies:**

**1. Skip MetricSummaryCallback in Development**
```python
# Development mode: Skip PDF generation
if args.dev_mode:
    callbacks = [metric_file_cb, aggregator_cb, time_cb]
else:
    callbacks = [metric_file_cb, aggregator_cb, summary_cb, publisher_cb, time_cb]
```

**Speedup**: 5-10 minutes (50% reduction)

---

**2. Reduce Histogram Bins**
```python
# Default: 20 bins (slow)
summary_cb = MetricSummaryCallback(..., num_bins=20)

# Fast mode: 10 bins
summary_cb = MetricSummaryCallback(..., num_bins=10)
```

**Speedup**: 2-3 minutes (30% reduction in PDF rendering)

---

**3. Delete Scenario Files Eagerly**
```python
# Delete as soon as aggregated (save disk space, speed up S3 upload)
metric_file_cb = MetricFileCallback(
    ...,
    delete_scenario_metric_files=True
)
```

**Speedup**: 1-2 minutes (fewer files to upload)

---

**4. Use S3 Transfer Acceleration**
```python
from botocore.config import Config

# Configure boto3 S3 client with transfer acceleration
s3_client = boto3.client('s3', config=Config(
    s3={'use_accelerate_endpoint': True}
))
publisher_cb = PublisherCallback(..., s3_client=s3_client)
```

**Speedup**: 2-5 minutes (faster uploads)

---

**5. Parallelize Independent Callbacks (Custom Implementation)**
```python
from concurrent.futures import ThreadPoolExecutor

class ParallelMainCallback(AbstractMainCallback):
    def __init__(self, callbacks: List[AbstractMainCallback]):
        self._callbacks = callbacks

    def on_run_simulation_end(self):
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(cb.on_run_simulation_end) for cb in self._callbacks]
            for future in futures:
                future.result()
```

**AIDEV-NOTE**: Only works if callbacks are independent (no shared file access)!

---

## Cross-References

### Phase 2C Documentation (This Phase)
- **nuplan/planning/simulation/callback/CLAUDE.md** - Per-scenario callbacks (8 hooks)
- **nuplan/planning/simulation/main_callback/test/CLAUDE.md** - Test suite for this module
- **nuplan/planning/simulation/runner/** - Simulation execution (calls main callbacks)
- **nuplan/planning/simulation/trajectory/** - Trajectory representations

### Related Modules
- **nuplan/planning/metrics/** - Metric computation, aggregation, file formats
- **nuplan/common/utils/s3_utils.py** - S3 utilities (path detection, client creation)
- **nuplan/common/utils/io_utils.py** - File I/O utilities

### Upstream Dependencies
- **nuplan/planning/simulation/** - Root simulation orchestrator
- **nuplan/planning/script/run_simulation.py** - Main entry point
- **nuplan/planning/script/builders/main_callback_builder.py** - Hydra instantiation

### External Resources
- **nuPlan documentation**: https://nuplan-devkit.readthedocs.io/
- **Boto3 S3 docs**: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html
- **Matplotlib PDF backend**: https://matplotlib.org/stable/api/backend_pdf_api.html

---

## Quick Reference

### File Manifest
```
main_callback/
â”œâ”€â”€ abstract_main_callback.py       (14 lines)  - Base interface
â”œâ”€â”€ multi_main_callback.py          (34 lines)  - Composite pattern
â”œâ”€â”€ time_callback.py                (28 lines)  - Runtime tracking
â”œâ”€â”€ metric_file_callback.py         (80 lines)  - Metric consolidation
â”œâ”€â”€ metric_aggregator_callback.py   (70 lines)  - Aggregator execution
â”œâ”€â”€ metric_summary_callback.py      (345 lines) - PDF report generation
â”œâ”€â”€ validation_callback.py          (54 lines)  - Pass/fail markers
â”œâ”€â”€ publisher_callback.py           (106 lines) - S3 uploads
â”œâ”€â”€ completion_callback.py          (46 lines)  - Competition tracking
â””â”€â”€ test/                           - Test suite
```

**Total LOC**: ~777 lines (excluding tests)

### Recommended Callback Orderings

**Local development:**
```python
[MetricFileCallback, MetricAggregatorCallback, TimeCallback]
```

**Full analysis:**
```python
[MetricFileCallback, MetricAggregatorCallback, MetricSummaryCallback, TimeCallback]
```

**Competition submission:**
```python
[MetricFileCallback, MetricAggregatorCallback, ValidationCallback, PublisherCallback, CompletionCallback, TimeCallback]
```

### Environment Variables

**Required for CompletionCallback:**
- `NUPLAN_SERVER_S3_ROOT_URL`: S3 bucket for competition

**Optional:**
- `SCENARIO_FILTER_ID`: Instance ID for parallel runs (default: "0")

### Common Commands

```bash
# Run simulation with main callbacks (Hydra config)
uv run python nuplan/planning/script/run_simulation.py \
    main_callback=default_main_callback

# Test main callbacks
just test-path nuplan/planning/simulation/main_callback/test

# Check generated outputs
ls -lh /tmp/exp/metrics/*.parquet
ls -lh /tmp/exp/summary/*.pdf
ls -lh /tmp/exp/validation/{passed,failed}.txt
```

---

## AIDEV Notes

### AIDEV-NOTE: Callback Abstraction Layers
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AbstractMainCallback (THIS MODULE)                  â”‚
â”‚   â”œâ”€ 2 hooks: start/end                             â”‚
â”‚   â”œâ”€ Process-level (once per simulation job)        â”‚
â”‚   â”œâ”€ Operates on aggregated results                 â”‚
â”‚   â””â”€ Use cases: aggregation, publishing, reporting  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Different abstraction level!
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AbstractCallback (callback/ module)                 â”‚
â”‚   â”œâ”€ 8 hooks: per-scenario lifecycle                â”‚
â”‚   â”œâ”€ Scenario-level (many times per job)            â”‚
â”‚   â”œâ”€ Operates on live simulation state              â”‚
â”‚   â””â”€ Use cases: metrics, serialization, profiling   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Don't confuse these two interfaces!**

---

### AIDEV-NOTE: Exception Handling Gap
MultiMainCallback has NO exception isolation - if one callback crashes, later callbacks don't run.

**Impact**: Partial results (e.g., metrics aggregated but not uploaded)

**TODO**: Add resilient wrapper or exception handling to MultiMainCallback

---

### AIDEV-NOTE: S3 Path Inconsistency
Some callbacks support S3 paths, others don't:
- âœ… MetricFileCallback, ValidationCallback, CompletionCallback
- âŒ MetricSummaryCallback (no S3 support - crashes on S3 paths!)

**TODO**: Add S3 support to MetricSummaryCallback or document limitation

---

### AIDEV-NOTE: Performance Bottleneck Ranking
Ranked by execution time (slowest first):
1. **MetricSummaryCallback** (5-10 min) - Matplotlib PDF rendering
2. **PublisherCallback** (2-10 min) - S3 uploads (network I/O)
3. **MetricAggregatorCallback** (1-2 min) - Aggregator computation
4. **MetricFileCallback** (30-60 sec) - Parquet I/O
5. **ValidationCallback** (1-5 sec) - Parquet read
6. **CompletionCallback** (<1 sec) - Trivial file write
7. **TimeCallback** (<1 ms) - Just logging

**Optimization priority**: Focus on MetricSummaryCallback (biggest impact).

---

### AIDEV-TODO: Refactoring Opportunities
1. Extract histogram rendering from MetricSummaryCallback â†’ separate module
2. Add exception isolation to MultiMainCallback
3. Implement streaming parquet aggregation (reduce memory)
4. Add S3 support to MetricSummaryCallback
5. Replace `Exception` catch-all with specific exceptions
6. Add callback dependency validation (DAG-based ordering)
7. Document recommended callback orderings in config templates
8. Add progress tracking for long-running callbacks (tqdm for all)
9. Implement parallel callback execution for independent callbacks
10. Add metadata to marker files (timestamp, version, etc.)

---

**End of Tier 2 Documentation - Phase 2C Session 5 Batch 2**

**Documentation Quality Checklist:**
- âœ… 16+ specific gotchas with examples
- âœ… Clear distinction from callback/ module
- âœ… Complete architecture diagrams
- âœ… Runnable code examples
- âœ… Performance benchmarks and optimizations
- âœ… Cross-references to related modules
- âœ… AIDEV notes for future improvements
- âœ… Module names only in dependencies section
- âœ… Quick reference section
